# Nebelus API Documentation

## Overview

The Nebelus API enables real-time voice conversations, bidirectional translation with automatic speaker diarization, and agent chat. Connect via WebSocket or REST API, send audio, receive transcriptions, translations, and agent responses.

**Key Points:**
- One WebSocket connection per client
- `session_id` is **auto-generated by the backend** when an agent/tool initiates translation
- Client receives `session_id` in `TRANSLATION_SESSION_START` event
- Send PCM audio chunks continuously (16-bit PCM @ 16kHz)
- Receive transcriptions and translations in real-time
- Automatic speaker diarization (S1, S2) and language detection
- Optional Text-to-Speech (TTS) for translated text

## Table of Contents

1. [Authentication](#authentication)
2. [Connection Flow](#connection-flow)
3. [Outgoing Events (Client → Server)](#outgoing-events-client--server)
4. [Incoming Events (Server → Client)](#incoming-events-server--client)
5. [Voice Conversation Events](#voice-conversation-events)
6. [Agent Recording Events](#agent-recording-events)
7. [Audio Format Requirements](#audio-format-requirements)
8. [Event Flow Diagram](#event-flow-diagram)
9. [Error Handling](#error-handling)
10. [Best Practices](#best-practices)

---

## Authentication

The WebSocket API uses **API Key authentication** for secure access.

### API Key Authentication

API keys with the `sk-ns-org-...` format are used for all WebSocket connections.

**Browser (query parameter):**
```javascript
const apiKey = 'sk-ns-org-xxxxx';
const ws = new WebSocket(`wss://api.nebelus.ai/stream/?api_key=${apiKey}`);
```

**Server-Side (with headers):**
```python
import websockets

async def connect():
    uri = "wss://api.nebelus.ai/stream/"
    headers = [
        ("Authorization", f"Bearer sk-ns-org-xxxxx"),
    ]
    async with websockets.connect(uri, extra_headers=headers) as ws:
        # Use WebSocket
        pass
```

**Node.js (with headers):**
```javascript
const WebSocket = require('ws');

const ws = new WebSocket('wss://api.nebelus.ai/stream/', {
    headers: {
        'Authorization': 'Bearer sk-ns-org-xxxxx'
    }
});
```

### Best Practices

- ✅ Keep API keys secure (never commit to version control)
- ✅ Use environment variables for API keys
- ✅ Rotate keys periodically for security
- ✅ Use HTTPS/WSS in production

### Authentication Errors

```json
{
  "type": "error",
  "message": "Authentication required"
}
```

**Failure codes:**
- HTTP upgrade rejected with `401` → missing or invalid credential
- WebSocket close code `4001` → authentication failed after handshake
- WebSocket close code `1011` → server error during connection

---

## Connection Flow

### 1. Establish WebSocket Connection

**Browser (using API key):**
```javascript
const apiKey = 'sk-ns-org-xxxxx';
const ws = new WebSocket(`wss://api.nebelus.ai/stream/?api_key=${apiKey}`);

ws.onopen = () => {
    console.log('Connected to Nebelus WebSocket');
};

ws.onerror = (error) => {
    console.error('WebSocket error:', error);
};

ws.onclose = (event) => {
    if (event.code === 4001) {
        console.error('Authentication failed - check your API key');
    } else {
        console.log('Disconnected:', event.code, event.reason);
    }
};
```

**Server-Side (Python with headers):**
```python
import asyncio
import json
import websockets

async def connect():
    uri = "wss://api.nebelus.ai/stream/"
    headers = [
        ("Authorization", f"Bearer sk-ns-org-xxxxx"),
    ]
    async with websockets.connect(uri, extra_headers=headers) as ws:
        await ws.send(json.dumps({"type": "message", "content": "Hello"}))
        async for message in ws:
            print("Received:", message)

asyncio.run(connect())
```

**Server-Side (Node.js with headers):**
```javascript
const WebSocket = require('ws');

const ws = new WebSocket('wss://api.nebelus.ai/stream/', {
    headers: {
        'Authorization': 'Bearer sk-ns-org-xxxxx'
    }
});

ws.on('open', () => {
    console.log('Connected to Nebelus WebSocket');
});

ws.on('error', (error) => {
    console.error('WebSocket error:', error);
});

ws.on('close', (code, reason) => {
    if (code === 4001) {
        console.error('Authentication failed - check your API key');
    } else {
        console.log('Disconnected:', code, reason.toString());
    }
});
```

### 2. Handle Incoming Messages

```javascript
ws.onmessage = (event) => {
    const data = JSON.parse(event.data)
    
    switch (data.event) {
        case 'TRANSLATION_SESSION_START':
            console.log('Session started:', data.content.session_id)
            break
        case 'TRANSLATION_RESULT':
            console.log('Translation:', data.content)
            break
        case 'TRANSLATION_COMPLETE':
            console.log('Session complete')
            break
        case 'TRANSLATION_SESSION_ERROR':
            console.error('Error:', data.content.error)
            break
    }
}
```

### 3. Heartbeat (Keep-Alive)

Send PING every 30-60 seconds to keep the connection alive:

```javascript
setInterval(() => {
    if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ event: 'PING' }));
    }
}, 30000);

// Server responds with:
// { "event": "PONG", "timestamp": "2025-11-11T..." }
```

**Python example:**
```python
import asyncio

async def heartbeat(ws):
    while True:
        await asyncio.sleep(30);
        if ws.open:
            await ws.send(json.dumps({"event": "PING"}));
```

---

## Outgoing Events (Client → Server)

### 1. TRANSLATION_UPDATE_SETTINGS

**When to send:** After receiving `TRANSLATION_SESSION_START`, to update translation session settings before streaming audio.

**Required fields:**
- `session_id`: Session ID received from `TRANSLATION_SESSION_START` event (auto-generated by backend)
- `language_a`: First language code (e.g., `"en"`)
- `language_b`: Second language code (e.g., `"es"`)

**Optional fields:**
- `context`: Text context to improve STT accuracy (max 10K chars)
- `endpoint_mode`: `"auto"` (default) or `"manual"`
  - `auto`: AI automatically detects end of speech
  - `manual`: Client sends `TRANSLATION_UTTERANCE_END` to mark end
- `enable_tts`: `true` or `false` (default: `true`)
- `voice_a`: TTS voice for language A (optional)
- `voice_b`: TTS voice for language B (optional)

**Example:**

```json
{
  "event": "TRANSLATION_SETTINGS_UPDATE",
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "language_a": "en",
  "language_b": "es",
  "context": "Medical consultation about diabetes treatment",
  "endpoint_mode": "auto",
  "enable_tts": true,
  "voice_a": "en-US-Neural2-A",
  "voice_b": "es-ES-Neural2-A"
}
```

**Server Response:** No explicit response. Settings are saved. Any errors trigger `TRANSLATION_SESSION_ERROR`.

---

### 2. TRANSLATION_AUDIO_STREAM

**When to send:** Continuously while user is speaking. Send audio chunks every 500ms - 1s.

**Required fields:**
- `session_id`: Session ID from `TRANSLATION_SETTINGS_UPDATE`
- `audio_data`: Base64-encoded PCM audio (16-bit PCM @ 16kHz)

**Audio format requirements:**
- Sample rate: 16,000 Hz (16kHz)
- Bit depth: 16-bit signed PCM
- Channels: Mono (1 channel)
- Encoding: Base64-encoded binary

**Example:**

```json
{
  "event": "TRANSLATION_AUDIO_STREAM",
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "audio_data": "AAABAAIAA..."
}
```

**How audio flows:**
1. Client captures microphone audio (48kHz, Float32)
2. Client resamples to 16kHz
3. Client converts Float32 → Int16 PCM
4. Client encodes to Base64
5. Client sends via WebSocket
6. Server processes audio for transcription/translation

**Server Response:** Server processes audio in background. Results come via `TRANSLATION_RESULT` events.

---

### 3. TRANSLATION_END_UTTERANCE

**When to send:** Only in **manual mode** (`endpoint_mode: "manual"`). Client signals end of speech.

**Required fields:**
- `session_id`: Session ID

**Example:**

```json
{
  "event": "TRANSLATION_UTTERANCE_END",
  "session_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Effect:** Flushes buffered audio, triggering immediate transcription/translation.

**Server Response:** Server processes buffered audio. Results come via `TRANSLATION_RESULT`.

---

### 4. TRANSLATION_END

**When to send:** When user stops translation session (e.g., clicks "Stop" button).

**Required fields:**
- `session_id`: Session ID

**Example:**

```json
{
  "event": "TRANSLATION_SESSION_END",
  "session_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Server Response:**

```json
{
  "event": "TRANSLATION_COMPLETE",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "status": "completed"
  }
}
```

**Effect:** Closes translation session, marks session as `completed` in database, stops audio processing.

---

## Incoming Events (Server → Client)

### 1. TRANSLATION_START

**When you receive it:** Server sends this when an agent/tool initiates a translation session. The backend **auto-generates** the `session_id` when creating the session.

**Payload:**

```json
{
  "event": "TRANSLATION_SESSION_START",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "language_a": "en",
    "language_b": "es",
    "instruction_message": "Translation session started. Speak naturally.",
    "enable_tts": true,
    "show_ui": true
  }
}
```

**Fields:**
- `session_id`: **Auto-generated UUID** by backend - use this for all subsequent translation operations
- `language_a`: First language code
- `language_b`: Second language code
- `instruction_message`: Message to display to user
- `enable_tts`: Whether TTS is enabled
- `show_ui`: Whether to show translation UI

**What to do:** 
1. **Store the `session_id`** - you'll need it for all translation events
2. Display translation UI
3. Optionally send `TRANSLATION_SETTINGS_UPDATE` to modify settings
4. Prepare to send audio via `TRANSLATION_AUDIO_STREAM`

---

### 2. TRANSLATION_RESULT

**When you receive it:** Every time the server completes transcription + translation of an utterance.

**Payload:**

```json
{
  "event": "TRANSLATION_RESULT",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "speaker": "S1",
    "original_text": "How are you feeling today?",
    "original_language": "en",
    "translated_text": "¿Cómo te sientes hoy?",
    "target_language": "es",
    "confidence": 0.95,
    "tts_audio": "UklGRiQAAABXQVZFZm10IBAAA..."
  }
}
```

**Fields:**
- `session_id`: Session ID
- `speaker`: Speaker identifier (`"S1"`, `"S2"`, etc.) from diarization
- `original_text`: Transcribed text in original language
- `original_language`: Detected language code
- `translated_text`: Translated text
- `target_language`: Target language code
- `confidence`: Transcription confidence (0.0 - 1.0)
- `tts_audio`: Base64-encoded WAV audio (if `enable_tts: true`), or `null`

**What to do:** 
1. Display original and translated text
2. If `tts_audio` present, decode Base64 and play audio

**Example - Playing TTS Audio:**

```javascript
function playTTSAudio(base64Audio) {
    const audioBytes = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0))
    const audioBlob = new Blob([audioBytes], { type: 'audio/wav' })
    const audioUrl = URL.createObjectURL(audioBlob)
    const audio = new Audio(audioUrl)
    audio.play()
}
```

---

### 3. TRANSLATION_COMPLETE

**When you receive it:** After client sends `TRANSLATION_SESSION_END`, confirming session closure.

**Payload:**

```json
{
  "event": "TRANSLATION_COMPLETE",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "status": "completed"
  }
}
```

**What to do:** Hide translation UI, stop audio capture, clean up resources.

---

### 4. TRANSLATION_ERROR

**When you receive it:** When any error occurs during translation.

**Payload:**

```json
{
  "event": "TRANSLATION_SESSION_ERROR",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "error": "Translation session not found"
  }
}
```

**Common errors:**
- `"Translation session not found"` - Invalid `session_id`
- `"Invalid audio data encoding"` - Malformed Base64 audio
- `"Failed to start translation service"` - Translation service connection failure

**What to do:** Display error to user, optionally retry or end session.

---

## Voice Conversation Events

The Nebelus WebSocket API supports real-time voice conversations with AI agents. These events are separate from translation events and handle the full voice conversation flow including transcription, agent responses, and text-to-speech.

### Event Flow Overview

Voice conversations follow this general flow:

1. **Session Creation**: `VOICE_SESSION_CREATED` - Backend creates a voice session
2. **Session State**: `VOICE_SESSION_STATE_CHANGED` - Session state updates (idle, listening, thinking, speaking)
3. **Voice Turns**: `VOICE_TURN_START` / `VOICE_TURN_END` - User speech turns
4. **Agent Response**: `VOICE_AGENT_RESPONSE_START` / `VOICE_AGENT_RESPONSE_STREAM` / `VOICE_AGENT_RESPONSE_END` - Agent processing
5. **TTS Audio**: `VOICE_OUTPUT_START` / `VOICE_OUTPUT_AUDIO` / `VOICE_OUTPUT_END` - Text-to-speech playback
6. **Message Creation**: `VOICE_MESSAGE_CREATED` - Messages created from voice turns

### Voice Session Events

#### VOICE_SESSION_CREATED

**When you receive it:** When a voice conversation session is initialized by the backend.

**Payload:**

```json
{
  "event": "VOICE_SESSION_CREATED",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "agent_id": 123,
    "thread_id": "thread_abc123",
    "interaction_mode": "conversation",
    "created_at": "2025-01-15T10:30:00Z"
  }
}
```

**Fields:**
- `session_id`: Unique session identifier (UUID)
- `agent_id`: ID of the agent handling the conversation
- `thread_id`: Thread ID for message history
- `interaction_mode`: Mode of interaction (`"conversation"`, `"transcribe_only"`, `"transcribe_turns"`)
- `created_at`: ISO 8601 timestamp

**What to do:** Store the `session_id` for all subsequent voice events. Initialize UI for voice conversation.

---

#### VOICE_SESSION_STATE_CHANGED

**When you receive it:** When the voice session state changes (e.g., idle → listening → thinking → speaking).

**Payload:**

```json
{
  "event": "VOICE_SESSION_STATE_CHANGED",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "state": "listening",
    "previous_state": "idle",
    "timestamp": "2025-01-15T10:30:05Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `state`: Current state (`"idle"`, `"listening"`, `"thinking"`, `"speaking"`, `"error"`)
- `previous_state`: Previous state before change
- `timestamp`: ISO 8601 timestamp

**What to do:** Update UI to reflect current state (e.g., show microphone indicator when `"listening"`, show thinking indicator when `"thinking"`).

---

#### VOICE_SESSION_ENDED

**When you receive it:** When the voice session is terminated.

**Payload:**

```json
{
  "event": "VOICE_SESSION_ENDED",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "reason": "user_ended",
    "ended_at": "2025-01-15T10:40:00Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `reason`: Reason for ending (`"user_ended"`, `"timeout"`, `"error"`, `"agent_ended"`)
- `ended_at`: ISO 8601 timestamp

**What to do:** Clean up UI, stop audio capture, release microphone resources.

---

### Voice Turn Events

#### VOICE_TURN_START

**When you receive it:** When a new voice turn begins (user starts speaking).

**Payload:**

```json
{
  "event": "VOICE_TURN_START",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "turn_id": "turn_xyz789",
    "speaker": "user",
    "started_at": "2025-01-15T10:30:15Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `turn_id`: Unique turn identifier
- `speaker`: Speaker identifier (`"user"`, `"agent"`, or speaker diarization ID like `"S1"`, `"S2"`)
- `started_at`: ISO 8601 timestamp

**What to do:** Show visual indicator that user is speaking. Start accumulating audio for this turn.

---

#### VOICE_TURN_END

**When you receive it:** When a voice turn completes (user stops speaking and transcription is ready).

**Payload:**

```json
{
  "event": "VOICE_TURN_END",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "turn_id": "turn_xyz789",
    "speaker": "user",
    "text": "Hello, how are you today?",
    "language": "en",
    "confidence": 0.95,
    "started_at": "2025-01-15T10:30:15Z",
    "ended_at": "2025-01-15T10:30:18Z",
    "metadata": {
      "word_count": 5,
      "duration_ms": 3000
    }
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `turn_id`: Turn identifier (matches `VOICE_TURN_START`)
- `speaker`: Speaker identifier
- `text`: Transcribed text
- `language`: Detected language code
- `confidence`: Transcription confidence (0.0 - 1.0)
- `started_at`: Turn start timestamp
- `ended_at`: Turn end timestamp
- `metadata`: Additional metadata (word count, duration, etc.)

**What to do:** 
- Display transcribed text in UI
- In `transcribe_only` mode: Add text to message input or send message
- In `conversation` or `transcribe_turns` mode: Wait for `VOICE_MESSAGE_CREATED` event (backend handles message creation)

---

#### VOICE_TURN_INTERRUPT

**When you receive it:** When a voice turn is interrupted (e.g., user starts speaking again before previous turn completes).

**Payload:**

```json
{
  "event": "VOICE_TURN_INTERRUPT",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "turn_id": "turn_xyz789",
    "interrupted_by": "turn_abc456",
    "interrupted_at": "2025-01-15T10:30:17Z"
  }
}
```

**What to do:** Cancel any pending processing for the interrupted turn. Prepare for new turn.

---

### Agent Voice Response Events

#### AGENT_VOICE_RESPONSE_START

**When you receive it:** When the agent starts processing a voice turn and generating a response.

**Payload:**

```json
{
  "event": "VOICE_AGENT_RESPONSE_START",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "turn_id": "turn_xyz789",
    "response_id": "response_def456",
    "started_at": "2025-01-15T10:30:20Z"
  }
}
```

**What to do:** Show "thinking" or "processing" indicator in UI. Update session state to `"thinking"`.

---

#### AGENT_VOICE_RESPONSE_STREAM

**When you receive it:** When the agent response is being streamed (text chunks as agent generates response).

**Payload:**

```json
{
  "event": "VOICE_AGENT_RESPONSE_STREAM",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "text_chunk": "I'm doing well, thank you for asking.",
    "is_complete": false
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `response_id`: Response identifier (matches `VOICE_AGENT_RESPONSE_START`)
- `text_chunk`: Partial text chunk from agent
- `is_complete`: Whether this is the final chunk

**What to do:** Display streaming text in UI (append chunks as they arrive). In conversation mode, this text will be used for TTS.

---

#### AGENT_VOICE_RESPONSE_END

**When you receive it:** When the agent finishes generating a response.

**Payload:**

```json
{
  "event": "VOICE_AGENT_RESPONSE_END",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "text": "I'm doing well, thank you for asking. How can I help you today?",
    "completed_at": "2025-01-15T10:30:25Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `response_id`: Response identifier
- `text`: Complete response text
- `completed_at`: ISO 8601 timestamp

**What to do:** Finalize response text display. If TTS is enabled, wait for `VOICE_OUTPUT_START` event to begin audio playback.

---

#### AGENT_VOICE_RESPONSE_ERROR

**When you receive it:** When an error occurs during agent response generation.

**Payload:**

```json
{
  "event": "VOICE_AGENT_RESPONSE_ERROR",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "error": "Agent timeout",
    "error_code": "TIMEOUT",
    "occurred_at": "2025-01-15T10:30:30Z"
  }
}
```

**What to do:** Display error message to user. Optionally retry or end session.

---

### Text-to-Speech (TTS) Events

#### TTS_START

**When you receive it:** When TTS generation begins for an agent response.

**Payload:**

```json
{
  "event": "VOICE_OUTPUT_START",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "text": "I'm doing well, thank you for asking.",
    "voice": "en-US-Neural2-A",
    "started_at": "2025-01-15T10:30:26Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `response_id`: Response identifier (matches agent response)
- `text`: Text being converted to speech
- `voice`: TTS voice identifier
- `started_at`: ISO 8601 timestamp

**What to do:** Prepare audio playback. Update session state to `"speaking"`.

---

#### TTS_AUDIO_CHUNK

**When you receive it:** When a chunk of TTS audio is ready for playback.

**Payload:**

```json
{
  "event": "VOICE_OUTPUT_AUDIO",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "audio_data": "UklGRiQAAABXQVZFZm10IBAAA...",
    "chunk_index": 0,
    "is_final": false
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `response_id`: Response identifier
- `audio_data`: Base64-encoded WAV audio chunk
- `chunk_index`: Sequential chunk index (0, 1, 2, ...)
- `is_final`: Whether this is the final chunk

**What to do:** 
1. Decode Base64 audio data
2. Queue audio chunk for playback
3. Play chunks sequentially as they arrive

**Example - Playing TTS Audio Chunks:**

```javascript
let ttsAudioQueue = []
let isPlayingTTS = false

function handleTTSAudioChunk(content) {
  // Decode Base64 to audio buffer
  const audioBytes = Uint8Array.from(atob(content.audio_data), c => c.charCodeAt(0))
  const audioBlob = new Blob([audioBytes], { type: 'audio/wav' })
  const audioUrl = URL.createObjectURL(audioBlob)
  
  // Queue for playback
  ttsAudioQueue.push(audioUrl)
  
  // Start playing if not already playing
  if (!isPlayingTTS) {
    playNextTTSChunk()
  }
}

function playNextTTSChunk() {
  if (ttsAudioQueue.length === 0) {
    isPlayingTTS = false
    return
  }
  
  isPlayingTTS = true
  const audioUrl = ttsAudioQueue.shift()
  const audio = new Audio(audioUrl)
  
  audio.onended = () => {
    URL.revokeObjectURL(audioUrl)
    playNextTTSChunk()
  }
  
  audio.onerror = () => {
    console.error('TTS audio playback error')
    URL.revokeObjectURL(audioUrl)
    playNextTTSChunk()
  }
  
  audio.play()
}
```

---

#### TTS_END

**When you receive it:** When TTS generation completes for a response.

**Payload:**

```json
{
  "event": "VOICE_OUTPUT_END",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "total_chunks": 5,
    "completed_at": "2025-01-15T10:30:28Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `response_id`: Response identifier
- `total_chunks`: Total number of audio chunks generated
- `completed_at`: ISO 8601 timestamp

**What to do:** Wait for queued audio to finish playing. Update session state back to `"listening"` or `"idle"`.

---

#### TTS_ERROR

**When you receive it:** When an error occurs during TTS generation.

**Payload:**

```json
{
  "event": "VOICE_OUTPUT_ERROR",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "response_id": "response_def456",
    "error": "TTS service unavailable",
    "error_code": "SERVICE_UNAVAILABLE",
    "occurred_at": "2025-01-15T10:30:27Z"
  }
}
```

**What to do:** Log error. Optionally fall back to text-only display. Continue conversation without audio.

---

### Message Events

#### VOICE_MESSAGE_CREATED

**When you receive it:** When a message is created from a completed voice turn. This event is sent in `conversation` and `transcribe_turns` modes where the backend automatically creates messages.

**Payload:**

```json
{
  "event": "VOICE_MESSAGE_CREATED",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "turn_id": "turn_xyz789",
    "message_id": "msg_123456",
    "thread_id": "thread_abc123",
    "role": "user",
    "content": "Hello, how are you today?",
    "metadata": {
      "source": "voice_turn",
      "voice_turn": {
        "session_id": "550e8400-e29b-41d4-a716-446655440000",
        "turn_id": "turn_xyz789",
        "speaker": "user",
        "language": "en",
        "confidence": 0.95
      }
    },
    "created_at": "2025-01-15T10:30:18Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `turn_id`: Turn identifier (matches `VOICE_TURN_END`)
- `message_id`: Unique message identifier
- `thread_id`: Thread identifier for message history
- `role`: Message role (`"user"` or `"assistant"`)
- `content`: Message text content
- `metadata`: Additional metadata including voice turn information
- `created_at`: ISO 8601 timestamp

**What to do:** 
- Display message in chat timeline/UI
- Load thread if not already loaded
- In `conversation` mode: This is sent automatically by backend, don't manually send messages
- In `transcribe_turns` mode: Messages are created for each turn automatically

**Important:** In `conversation` and `transcribe_turns` modes, do NOT manually send messages via the send API when receiving `VOICE_TURN_END`. The backend handles message creation automatically via this event.

---

### Agent Recording Events

The system supports agent-initiated audio recording, where the AI agent can request to record audio from the user for a specified duration. This is useful for voice memos, audio samples, or other scenarios where the agent needs to capture audio.

#### AGENT_RECORDING_START

**When you receive it:** When the agent initiates an audio recording session.

**Payload:**

```json
{
  "event": "AGENT_RECORDING_START",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "thread_id": "thread_abc123",
    "duration_seconds": 30,
    "instruction_message": "Please record your audio now",
    "enable_analysis": true,
    "language": "en"
  }
}
```

**Fields:**
- `session_id`: Unique recording session identifier (UUID)
- `thread_id`: Thread ID for message context
- `duration_seconds`: Maximum recording duration in seconds
- `instruction_message`: Message to display to user
- `enable_analysis`: Whether to enable audio analysis
- `language`: Expected language for transcription

**What to do:** 
- Start capturing audio from the user's microphone
- Display the instruction message to the user
- Set a timer for the duration limit
- Stream audio chunks to server via `AGENT_RECORDING_AUDIO_STREAM`

---

#### AGENT_RECORDING_AUDIO_STREAM

**When to send:** Continuously while recording audio, similar to translation audio streaming.

**Required fields:**
- `session_id`: Session ID from `AGENT_RECORDING_START`
- `audio_data`: Base64-encoded PCM audio (16-bit PCM @ 16kHz)

**Example:**

```json
{
  "event": "AGENT_RECORDING_AUDIO_STREAM",
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "audio_data": "AAABAAIAA..."
}
```

**Note:** This event is sent **by the client** to the server, not received from the server. Use the same audio format as translation streaming (16-bit PCM @ 16kHz).

---

#### AGENT_RECORDING_COMPLETE

**When you receive it:** When the agent recording session completes (either duration limit reached or manually stopped).

**Payload:**

```json
{
  "event": "AGENT_RECORDING_COMPLETE",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "audio_data": "UklGRiQAAABXQVZFZm10IBAAA...",
    "duration_ms": 25430
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `audio_data`: Complete audio recording as Base64-encoded WAV (optional)
- `duration_ms`: Actual recording duration in milliseconds

**What to do:** Stop audio capture, clean up resources, wait for transcription.

---

#### AGENT_RECORDING_TRANSCRIPTION

**When you receive it:** When the recorded audio has been transcribed.

**Payload:**

```json
{
  "event": "AGENT_RECORDING_TRANSCRIPTION",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "text": "This is the transcribed text from the recording",
    "language": "en",
    "confidence": 0.96
  }
}
```

**Fields:**
- `session_id`: Session identifier
- `text`: Transcribed text from the recording
- `language`: Detected language code
- `confidence`: Transcription confidence (0.0 - 1.0)

**What to do:** Display the transcription result to the user. The agent will typically process this text and respond.

---

#### AGENT_RECORDING_ERROR

**When you receive it:** When an error occurs during agent-initiated recording.

**Payload:**

```json
{
  "event": "AGENT_RECORDING_ERROR",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "error": "Recording timeout",
    "error_code": "TIMEOUT"
  }
}
```

**Fields:**
- `session_id`: Session identifier (may be null if error occurs before session creation)
- `error`: Human-readable error message
- `error_code`: Machine-readable error code

**Common error codes:**
- `TIMEOUT`: Recording exceeded maximum duration
- `MICROPHONE_DENIED`: User denied microphone access
- `SESSION_NOT_FOUND`: Invalid or expired session ID
- `TRANSCRIPTION_FAILED`: Audio transcription failed

**What to do:** Display error message to user, stop recording, clean up resources.

---

### Error Events

#### VOICE_ERROR

**When you receive it:** When a general error occurs in the voice conversation system.

**Payload:**

```json
{
  "event": "VOICE_ERROR",
  "content": {
    "session_id": "550e8400-e29b-41d4-a716-446655440000",
    "error": "Microphone access denied",
    "error_code": "MICROPHONE_DENIED",
    "error_type": "audio_error",
    "occurred_at": "2025-01-15T10:30:12Z"
  }
}
```

**Fields:**
- `session_id`: Session identifier (may be null if error occurs before session creation)
- `error`: Human-readable error message
- `error_code`: Machine-readable error code
- `error_type`: Error category (`"audio_error"`, `"session_error"`, `"transcription_error"`, etc.)
- `occurred_at`: ISO 8601 timestamp

**Common error codes:**
- `MICROPHONE_DENIED`: User denied microphone access
- `MICROPHONE_IN_USE`: Microphone already in use by another application
- `SESSION_NOT_FOUND`: Invalid or expired session ID
- `TRANSCRIPTION_FAILED`: Speech-to-text conversion failed
- `AGENT_ERROR`: Agent processing error

**What to do:** Display user-friendly error message. Provide recovery options (retry, end session, etc.).

---

### Voice Conversation Event Flow Diagram

```
CLIENT                                  SERVER                                AGENT
  |                                        |                                     |
  |--- WebSocket Connect ----------------->|                                     |
  |    (with auth token)                   |                                     |
  |                                        |                                     |
  |<-- Connection Accepted ----------------|                                     |
  |                                        |                                     |
  |                                        |<-- START VOICE CONVERSATION --------|
  |                                        |   (creates session)                 |
  |<-- VOICE_SESSION_CREATED --------------|                                     |
  |    (session_id: auto-generated UUID)   |                                     |
  |                                        |                                     |
  |<-- VOICE_SESSION_STATE_CHANGED --------|                                     |
  |    (state: "listening")                |                                     |
  |                                        |                                     |
  |--- Audio Stream (continuous) --------->|                                     |
  |    (16-bit PCM @ 16kHz)                |                                     |
  |                                        |                                     |
  |<-- VOICE_TURN_START -------------------|                                     |
  |    (turn_id, speaker: "user")          |                                     |
  |                                        |                                     |
  |--- Audio Stream (continued) ---------->|                                     |
  |                                        |                                     |
  |<-- VOICE_TURN_END ---------------------|                                     |
  |    (text: "Hello, how are you?")       |                                     |
  |                                        |                                     |
  |<-- VOICE_MESSAGE_CREATED --------------|                                     |
  |    (message_id, content)               |                                     |
  |                                        |                                     |
  |                                        |<-- PROCESS USER MESSAGE ------------|
  |                                        |                                     |
  |<-- AGENT_VOICE_RESPONSE_START ---------|                                     |
  |<-- VOICE_SESSION_STATE_CHANGED --------|                                     |
  |    (state: "thinking")                 |                                     |
  |                                        |                                     |
  |<-- AGENT_VOICE_RESPONSE_STREAM --------|                                     |
  |    (text_chunk: "I'm doing well...")   |                                     |
  |                                        |                                     |
  |<-- AGENT_VOICE_RESPONSE_END -----------|                                     |
  |    (text: "I'm doing well, thank you") |                                     |
  |                                        |                                     |
  |<-- VOICE_MESSAGE_CREATED --------------|                                     |
  |    (role: "assistant", content)        |                                     |
  |                                        |                                     |
  |<-- TTS_START --------------------------|                                     |
  |<-- VOICE_SESSION_STATE_CHANGED --------|                                     |
  |    (state: "speaking")                 |                                     |
  |                                        |                                     |
  |<-- TTS_AUDIO_CHUNK --------------------|                                     |
  |    (audio_data: Base64 WAV)            |                                     |
  |    [Play audio chunk]                  |                                     |
  |                                        |                                     |
  |<-- TTS_AUDIO_CHUNK --------------------|                                     |
  |    [Play audio chunk]                  |                                     |
  |                                        |                                     |
  |<-- TTS_END ----------------------------|                                     |
  |<-- VOICE_SESSION_STATE_CHANGED --------|                                     |
  |    (state: "listening")                |                                     |
  |                                        |                                     |
  |--- Audio Stream (next turn) ---------->|                                     |
  |                                        |                                     |
  |                                        |<-- END CONVERSATION ----------------|
  |<-- VOICE_SESSION_ENDED ----------------|                                     |
  |                                        |                                     |
```

---

### Agent Recording Best Practices

1. **Audio Capture**
   - Start recording immediately upon receiving `AGENT_RECORDING_START`
   - Use the same audio format as translation (16-bit PCM @ 16kHz)
   - Respect the `duration_seconds` limit
   - Stop recording when duration is reached or on `AGENT_RECORDING_COMPLETE`

2. **User Experience**
   - Display the `instruction_message` prominently to the user
   - Show a recording indicator (visual or audio cue)
   - Show a countdown timer for the recording duration
   - Provide a manual stop button if needed

3. **Error Handling**
   - Handle `AGENT_RECORDING_ERROR` gracefully
   - Provide clear feedback if microphone access is denied
   - Allow users to retry if recording fails

4. **Resource Management**
   - Clean up audio resources after recording completes
   - Stop microphone access when not needed
   - Release AudioContext and MediaStream properly

---

### Voice Conversation Best Practices

1. **Session Management**
   - Store `session_id` from `VOICE_SESSION_CREATED` immediately
   - Handle `VOICE_SESSION_ENDED` to clean up resources
   - Don't create your own session IDs - always use server-provided ones

2. **State Management**
   - Track session state via `VOICE_SESSION_STATE_CHANGED` events
   - Update UI indicators based on state (listening, thinking, speaking)
   - Don't send audio when state is not `"listening"`

3. **Audio Streaming**
   - Stream audio continuously while session is active
   - Use 16-bit PCM @ 16kHz format (same as translation)
   - Stop streaming when `VOICE_SESSION_STATE_CHANGED` indicates non-listening state

4. **Message Handling**
   - In `conversation` mode: Wait for `VOICE_MESSAGE_CREATED` events, don't manually send
   - In `transcribe_turns` mode: Messages are auto-created, display them as they arrive
   - In `transcribe_only` mode: Handle `VOICE_TURN_END` to manually create messages

5. **TTS Playback**
   - Queue TTS audio chunks and play sequentially
   - Don't interrupt playback unless user explicitly stops
   - Clean up audio URLs after playback to prevent memory leaks

6. **Error Handling**
   - Handle `VOICE_ERROR` events gracefully
   - Provide user-friendly error messages
   - Allow users to retry or end session on errors

---

### Agent Recording Event Flow

Agent-initiated recording is useful for capturing audio samples, voice memos, or other scenarios where the agent needs specific audio input:

```
CLIENT                                  SERVER                                AGENT
  |                                        |                                     |
  |                                        |<-- REQUEST AUDIO RECORDING ---------|
  |                                        |   (agent tool call)                 |
  |<-- AGENT_RECORDING_START --------------|                                     |
  |    (session_id, duration, instructions)|                                     |
  |                                        |                                     |
  |    [Start audio capture]               |                                     |
  |    [Display instructions]              |                                     |
  |    [Show recording indicator]          |                                     |
  |                                        |                                     |
  |--- AGENT_RECORDING_AUDIO_STREAM ------>|                                     |
  |    (Base64 PCM audio chunks)           |                                     |
  |                                        |                                     |
  |--- AGENT_RECORDING_AUDIO_STREAM ------>|                                     |
  |                                        |                                     |
  |--- AGENT_RECORDING_AUDIO_STREAM ------>|                                     |
  |                                        |                                     |
  |    [Duration limit reached OR          |                                     |
  |     user manually stopped]             |                                     |
  |                                        |                                     |
  |<-- AGENT_RECORDING_COMPLETE ------------|                                     |
  |    (audio_data, duration_ms)           |                                     |
  |                                        |                                     |
  |    [Stop audio capture]                |                                     |
  |    [Clean up resources]                |                                     |
  |                                        |                                     |
  |                                        |--- Process Audio ----------------->|
  |                                        |   (transcription, analysis)         |
  |                                        |                                     |
  |<-- AGENT_RECORDING_TRANSCRIPTION ------|                                     |
  |    (text, language, confidence)        |                                     |
  |                                        |                                     |
  |    [Display transcription]             |                                     |
  |                                        |                                     |
  |                                        |<-- Process Transcription -----------|
  |                                        |   (agent continues conversation)    |
  |<-- AGENT_VOICE_RESPONSE_START ---------|                                     |
  |<-- AGENT_VOICE_RESPONSE_STREAM --------|                                     |
  |<-- AGENT_VOICE_RESPONSE_END -----------|                                     |
  |                                        |                                     |
```

**Key Differences from Voice Conversation:**
- Agent initiates the recording (not user)
- Fixed duration limit specified by agent
- Recording stops automatically after duration
- Typically used for specific audio capture tasks
- Agent processes the recording and continues conversation

---

## Audio Format Requirements

### Required Format

- **Sample Rate:** 16,000 Hz (16kHz)
- **Bit Depth:** 16-bit signed integer PCM
- **Channels:** Mono (1 channel)
- **Encoding:** Base64-encoded binary

### Browser Audio Capture

```javascript
// 1. Get microphone access
const stream = await navigator.mediaDevices.getUserMedia({ 
    audio: { 
        sampleRate: 16000, // Prefer 16kHz
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true
    } 
})

// 2. Create AudioContext
const audioContext = new AudioContext({ sampleRate: 16000 })
const source = audioContext.createMediaStreamSource(stream)

// 3. Create ScriptProcessorNode for audio processing
const processor = audioContext.createScriptProcessor(4096, 1, 1)

processor.onaudioprocess = (e) => {
    const float32Audio = e.inputBuffer.getChannelData(0)
    
    // Convert Float32 (-1.0 to 1.0) to Int16 PCM (-32768 to 32767)
    const int16Audio = new Int16Array(float32Audio.length)
    for (let i = 0; i < float32Audio.length; i++) {
        const s = Math.max(-1, Math.min(1, float32Audio[i]))
        int16Audio[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
    }
    
    // Encode to Base64
    const base64Audio = btoa(String.fromCharCode(...new Uint8Array(int16Audio.buffer)))
    
    // Send to server
    ws.send(JSON.stringify({
        event: 'TRANSLATION_AUDIO_STREAM',
        session_id: sessionId,
        audio_data: base64Audio
    }))
}

source.connect(processor)
processor.connect(audioContext.destination)
```

### Resampling (if needed)

If browser captures at different sample rate (e.g., 48kHz), resample to 16kHz:

```javascript
function resampleTo16kHz(float32Array, originalSampleRate) {
    const targetSampleRate = 16000
    const sampleRateRatio = originalSampleRate / targetSampleRate
    const newLength = Math.round(float32Array.length / sampleRateRatio)
    const result = new Float32Array(newLength)
    
    for (let i = 0; i < newLength; i++) {
        const srcIndex = i * sampleRateRatio
        const srcIndexFloor = Math.floor(srcIndex)
        const srcIndexCeil = Math.min(srcIndexFloor + 1, float32Array.length - 1)
        const t = srcIndex - srcIndexFloor
        
        // Linear interpolation
        result[i] = float32Array[srcIndexFloor] * (1 - t) + float32Array[srcIndexCeil] * t
    }
    
    return result
}
```

---

## Event Flow Diagram

```
CLIENT                                  SERVER
  |                                        |
  |--- WebSocket Connect ----------------->|
  |    (with API key)                      |
  |                                        |
  |<-- Connection Accepted ----------------|
  |                                        |
  |                                        |<-- AGENT INITIATES SESSION
  |                                        |   (creates session, generates ID)
  |<-- TRANSLATION_START ------------------|
  |    (session_id: auto-generated UUID)   |
  |                                        |
  |--- TRANSLATION_UPDATE_SETTINGS ------->|
  |    (session_id from TRANSLATION_START) |
  |                                        |
  |--- TRANSLATION_AUDIO_STREAM ---------->|
  |    (Base64 PCM audio)                  |
  |                                        |
  |--- TRANSLATION_AUDIO_STREAM ---------->|
  |                                        |
  |<-- TRANSLATION_RESULT -----------------|
  |    (original, translated, TTS audio)   |
  |                                        |
  |--- TRANSLATION_AUDIO_STREAM ---------->|
  |                                        |
  |<-- TRANSLATION_RESULT -----------------|
  |                                        |
  |--- TRANSLATION_END ------------------->|
  |                                        |
  |<-- TRANSLATION_COMPLETE ---------------|
  |                                        |
```

---

## Error Handling

### Connection Errors

```javascript
ws.onerror = (error) => {
    console.error('WebSocket error:', error)
    // Retry with exponential backoff
}

ws.onclose = (event) => {
    if (event.code === 4001) {
        console.error('Authentication failed')
        // Redirect to login
    } else if (event.code === 1006) {
        console.warn('Connection lost, reconnecting...')
        // Retry connection
    }
}
```

### Event Errors

```javascript
ws.onmessage = (event) => {
    const data = JSON.parse(event.data)
    
    if (data.event === 'TRANSLATION_SESSION_ERROR') {
        const error = data.content.error
        
        if (error.includes('not found')) {
            // Session expired or invalid
            console.error('Invalid session, creating new one')
            createNewSession()
        } else if (error.includes('audio data')) {
            // Audio encoding issue
            console.error('Invalid audio format, check encoding')
        } else {
            // Generic error
            console.error('Translation error:', error)
        }
    }
}
```

### Reconnection Strategy

```javascript
let reconnectAttempts = 0
const maxReconnectAttempts = 5

function reconnect() {
    if (reconnectAttempts >= maxReconnectAttempts) {
        console.error('Max reconnection attempts reached')
        return
    }
    
    const delay = Math.min(1000 * Math.pow(2, reconnectAttempts), 30000)
    reconnectAttempts++
    
    console.log(`Reconnecting in ${delay}ms (attempt ${reconnectAttempts})`)
    
    setTimeout(() => {
        connect()
    }, delay)
}
```

---

## Best Practices

### 1. Session Management

- **Session IDs are auto-generated:** Backend creates `session_id` when agent/tool initiates translation
- **Store session_id from TRANSLATION_START:** Use the `session_id` received in `TRANSLATION_SESSION_START` event
- **Don't generate your own:** Clients should never create session IDs - always use the one from the server
- **One session per translation:** Each `TRANSLATION_SESSION_START` event provides a unique session
- **End sessions properly:** Always send `TRANSLATION_SESSION_END` before disconnecting
- **Session timeout:** Sessions auto-expire after 30 minutes of inactivity

### 1.5. Credential Management

- **Prefer API keys for backend systems:** They support fine-grained scopes and are designed for server-to-server use
- **Send only one credential per request:** Multiple credentials can cause authentication issues
- **Store secrets securely:** Use environment variables, secret stores, or KMS. Never embed keys in client-side JavaScript
- **Log token prefixes only:** When debugging, log the first 6-8 characters (e.g., `sk-ns...`), never the full secret
- **Rotate credentials regularly:** Delete unused API keys and DRF tokens - revoked credentials immediately stop authenticating

### 2. Audio Streaming

- **Send chunks regularly:** Every 500ms - 1 second
- **Don't send too frequently:** Avoid sending < 100ms chunks (inefficient)
- **Buffer audio properly:** Accumulate audio between sends
- **Stop streaming when done:** Don't send audio after `TRANSLATION_SESSION_END`

### 3. Connection Management

- **Use HTTPS/WSS:** Always use secure WebSocket (`wss://`) in production
- **Implement heartbeat:** Send PING every 30-60 seconds to keep connection alive
- **Handle disconnections:** Implement exponential backoff reconnection
- **Clean up resources:** Close AudioContext, MediaStream when done
- **Handle auth failures:** WebSocket close code `4001` means authentication failed - check credentials

### 4. Performance

- **Reuse WebSocket connection:** Don't create new connection per session
- **Minimize encoding overhead:** Pre-allocate typed arrays
- **Decode TTS audio efficiently:** Reuse Audio elements
- **Throttle UI updates:** Don't update DOM for every audio chunk

### 5. Security

- **Store tokens securely:** Use environment variables, secret stores, or KMS - never hardcode
- **Never embed keys in client-side code:** API keys should only be used server-side
- **Rotate API keys regularly:** Generate new keys periodically and delete unused ones
- **Use short-lived JWTs:** Prefer JWT with short expiration for user sessions
- **Validate session IDs:** Ensure UUIDs are properly formatted
- **Use HTTPS/WSS only:** Never send credentials over unencrypted connections
- **Log token prefixes only:** When debugging, log only the first 6-8 characters of tokens

### 6. User Experience

- **Show connection status:** Display "Connecting...", "Connected", "Disconnected"
- **Display transcription confidence:** Show visual indicator for low confidence
- **Show speaker labels:** Distinguish S1 vs S2 in UI
- **Auto-retry on errors:** Silently retry transient errors
- **Show error messages:** User-friendly errors for permanent failures

### 7. Testing

- **Test with different languages:** Verify language pairs work
- **Test with background noise:** Ensure STT works in noisy environments
- **Test long sessions:** Verify sessions don't timeout prematurely
- **Test network interruptions:** Ensure reconnection works
- **Test audio quality:** Verify 16kHz PCM encoding is correct

---

## Support

For issues, questions, or feature requests:

- **Documentation:** [https://api.nebelus.ai](https://api.nebelus.ai/api/docs)
- **API Status:** [https://status.nebelus.ai](https://status.nebelus.ai)
- **Support Email:** [support@nebelus.com](mailto:support@nebelus.com)

---

## Changelog

### 2025-11-18

- Initial documentation release
